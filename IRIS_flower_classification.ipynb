{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "import apache_beam as beam\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"/home/jugs/PycharmProjects/DelvifyFWD/others/iris/iris_training.csv\"\n",
    "test_data = \"/home/jugs/PycharmProjects/DelvifyFWD/others/iris/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=train_data, target_dtype=np.int, features_dtype=np.float32)\n",
    "testing_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=test_data, target_dtype=np.int, features_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "   'petal_lenght',\n",
    "   'petal_width',\n",
    "   'sepal_length',\n",
    "   'sepal_width',\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW_DATA_FEATURE_SPEC = dict([(name, tf.io.FixedLenFeature([], tf.float32)) for name in NUMERIC_FEATURE_KEYS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
    "#     schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW_DATA_FEATURE_SPEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW_DATA_METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class MapAndFilterErrors(beam.PTransform):\n",
    "  \"\"\"Like beam.Map but filters out erros in the map_fn.\"\"\"\n",
    "  class _MapAndFilterErrorsDoFn(beam.DoFn):\n",
    "    \"\"\"Count the bad examples using a beam metric.\"\"\"\n",
    "    def __init__(self, fn):\n",
    "        self._fn = fn\n",
    "      # Create a counter to measure number of bad elements.\n",
    "        self._bad_elements_counter = beam.metrics.Metrics.counter(\n",
    "            'census_example', 'bad_elements')\n",
    "    def process(self, element):\n",
    "        try:\n",
    "            yield self._fn(element)\n",
    "        except Exception:  # pylint: disable=broad-except\n",
    "        # Catch any exception the above call.\n",
    "            self._bad_elements_counter.inc(1)\n",
    "\n",
    "    def __init__(self, fn):\n",
    "        self._fn = fn\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "        return pcoll | beam.ParDo(self._MapAndFilterErrorsDoFn(self._fn))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def transform_data(train_data_file, test_data_file, working_dir):\n",
    "    def preprocessing_fn(inputs):\n",
    "        outputs = inputs.copy()\n",
    "        for key in NUMERIC_FEATURE_KEYS:\n",
    "            outputs[key] = tft.scale_to_0_1(outputs[key])\n",
    "        \n",
    "        table_keys = ['a', 'b', 'c']\n",
    "        initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "          keys=table_keys,\n",
    "          values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
    "          key_dtype=tf.string,\n",
    "          value_dtype=tf.int64)\n",
    "        table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
    "        outputs[LABEL_KEY] = table.lookup(outputs[LABEL_KEY])\n",
    "        return outputs\n",
    "    \n",
    "    with beam.Pipeline() as pipeline:\n",
    "        with tft_beam.Context(temp_dir=tempfile.mktemp()):\n",
    "            ordered_columns = [\n",
    "              'petal_lenght','petal_width','sepal_length','sepal_width',\n",
    "          ]\n",
    "        \n",
    "            converter = tft.coders.CsvCoder(ordered_columns, RAW_DATA_METADATA.schema)\n",
    "\n",
    "            raw_data = (\n",
    "              pipeline\n",
    "              | \"ReadTrainData\" >> beam.io.ReadFromText(train_data_file)\n",
    "              | \"FixCommasTrainData\" >> beam.Map(lambda line: line.replace(', ', ',')))\n",
    "            \n",
    "\n",
    "            raw_dataset = (raw_data, RAW_DATA_METADATA)\n",
    "            transformed_dataset , transform_fn = (raw_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "            transformed_data_coder = tft.coders.ExampleProtoCoder(transformed_metadata.schema)\n",
    "\n",
    "            _ = (\n",
    "              transformed_data\n",
    "              | \"EncodeTrainData\" >> beam.Map(transformed_data_coder.encode)\n",
    "              | \"WriteTrainData\" >> beam.io.WriteToTFRecord(os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE))\n",
    "               )\n",
    "\n",
    "            raw_test_data = (\n",
    "              pipeline\n",
    "              | \"ReadTestData\" >> beam.io.ReadFromText(test_data_file, skip_header_lines=1)\n",
    "              | \"FixCommasTestData\" >> beam.Map(lambda line: line.replace(\", \", \",\"))\n",
    "              | \"RemoveTrailingPeriodsTestData\" >> beam.Map(lambda line: line[:-1])\n",
    "              | \"DecodeTestData\" >> MapAndFilterErrors(converter.decode)\n",
    "             )\n",
    "\n",
    "            raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n",
    "            trasformed_test_dataset = ((raw_test_dataset, transform_fn) | tft_beam.TransformDataset())\n",
    "            transformed_test_data, _ = trasformed_test_dataset\n",
    "\n",
    "            _ = (\n",
    "              transformed_test_data\n",
    "              | \"EncodeTestData\" >> beam.Map(transformed_data_coder.encode)\n",
    "              | \"WriteTestData\" >> beam.io.WriteToTFRecord(os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE))\n",
    "            )\n",
    "\n",
    "            _ = (\n",
    "              transform_fn\n",
    "              | \"WriteTransformFn\" >> tft_beam.WriteTransformFn(working_dir)\n",
    "            )\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def _make_training_input_fn(tf_transform_output, transformed_examples, batch_size):\n",
    "    def input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern=transformed_examples,\n",
    "            batch_size=batch_size,\n",
    "            features=tf_transform_output.transformed_feature_spec(),\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            shuffle=True\n",
    "        )\n",
    "        transformed_features = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "        transformed_labels = transformed_features.pop(LABEL_KEY)\n",
    "        return transformed_features, transformed_labels\n",
    "    return input_fn\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def _make_training_input_fn(tf_transform_output, transformed_examples, batch_size):\n",
    "    def input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern=transformed_examples,\n",
    "            batch_size=batch_size,\n",
    "            features=tf_transform_output.transformed_feature_spec(),\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            shuffle=True\n",
    "        )\n",
    "        transformed_features = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "        transformed_labels = transformed_features.pop(LABEL_KEY)\n",
    "        return transformed_features, transformed_labels\n",
    "    return input_fn\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def _make_training_input_fn(tf_transform_output, transformed_examples, batch_size):\n",
    "    def input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern=transformed_examples,\n",
    "            batch_size=batch_size,\n",
    "            features=tf_transform_output.transformed_feature_spec(),\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            shuffle=True\n",
    "        )\n",
    "        transformed_features = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "        transformed_labels = transformed_features.pop(LABEL_KEY)\n",
    "        return transformed_features, transformed_labels\n",
    "    return input_fn\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def train_and_evaluate(working_dir, ):\n",
    "    tf_transformed_output = tft.TFTransformOutput(working_dir)\n",
    "    run_config = tf.estimator.RunConfig()\n",
    "\n",
    "    estimator = tf.estimator.LinearClassifier(\n",
    "        feature_columns=get_feature_columns(tf_transformed_output),\n",
    "        config=run_config,\n",
    "        loss_reduction=tf.losses.Reduction.SUM\n",
    "    )\n",
    "\n",
    "    train_input_fn = _make_training_input_fn(\n",
    "        tf_transformed_output,\n",
    "        os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
    "        batch_size=TRAIN_BATCH_SIZE\n",
    "    )\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=100)\n",
    "\n",
    "    eval_input_fn = _make_training_input_fn(\n",
    "        tf_transformed_output,\n",
    "        os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    serving_input_fn = _make_serving_input_fn(tf_transformed_output)\n",
    "    exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
    "    estimator.export_savedmodel(exported_model_dir, serving_input_fn)\n",
    "\n",
    "    return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "working_dir = '/tmp/iris'\n",
    "train_data = \"/home/jugs/PycharmProjects/DelvifyFWD/others/iris/iris_training.csv\"\n",
    "test_data = \"/home/jugs/PycharmProjects/DelvifyFWD/others/iris/iris_test.csv\"\n",
    "\n",
    "transform_data(train_data, test_data, working_dir)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                        hidden_units=[10,30,10], \n",
    "                                        n_classes=3, model_dir=\"/tmp/iris/iris_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(x=training_set.data, y=training_set.target, max_steps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_training_input_fn():\n",
    "    def input_fn():\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score = classifier.evaluate(x=testing_set.data, y=testing_set.target)['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {0:f}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = np.array([[6.4, 3.2, 4.5, 1.5], [6.4, 2.8, 5.6, 2.7], [5.8, 3.1, 5.0, 1.7]], dtype=float)\n",
    "\n",
    "y = list(classifier.predict(new_samples, as_iterable=True))\n",
    "print('Predictions: {}'.format(str(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "tf.saved_model.simple_save(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = tf.keras.utils.get_file(\n",
    "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
    "test_path = tf.keras.utils.get_file(\n",
    "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
    "\n",
    "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth  Species\n",
       "0          6.4         2.8          5.6         2.2        2\n",
       "1          5.0         2.3          3.3         1.0        1\n",
       "2          4.9         2.5          4.5         1.7        2\n",
       "3          4.9         3.1          1.5         0.1        0\n",
       "4          5.7         3.8          1.7         0.3        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          6.4         2.8          5.6         2.2\n",
       "1          5.0         2.3          3.3         1.0\n",
       "2          4.9         2.5          4.5         1.7\n",
       "3          4.9         3.1          1.5         0.1\n",
       "4          5.7         3.8          1.7         0.3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train.pop('Species')\n",
    "test_y = test.pop('Species')\n",
    "\n",
    "# The label column has now been removed from the features.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_evaluation_set():\n",
    "    features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "                'SepalWidth':  np.array([2.8, 2.3]),\n",
    "                'PetalLength': np.array([5.6, 3.3]),\n",
    "                'PetalWidth':  np.array([2.2, 1.0])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    \"\"\"An input function for training or evaluating\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle and repeat if you are in training mode.\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp9cv7ngir\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp9cv7ngir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f99d7890a90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    hidden_units=[30, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From /home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp9cv7ngir/model.ckpt.\n",
      "INFO:tensorflow:loss = 427.8204, step = 1\n",
      "INFO:tensorflow:global_step/sec: 597.94\n",
      "INFO:tensorflow:loss = 41.41667, step = 101 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 805.273\n",
      "INFO:tensorflow:loss = 23.701096, step = 201 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 791.671\n",
      "INFO:tensorflow:loss = 17.957558, step = 301 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 630.973\n",
      "INFO:tensorflow:loss = 18.874794, step = 401 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 751.258\n",
      "INFO:tensorflow:loss = 13.298775, step = 501 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 692.97\n",
      "INFO:tensorflow:loss = 14.072231, step = 601 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 737.695\n",
      "INFO:tensorflow:loss = 13.226655, step = 701 (0.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 748.635\n",
      "INFO:tensorflow:loss = 13.29369, step = 801 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 702.559\n",
      "INFO:tensorflow:loss = 12.909085, step = 901 (0.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 733.253\n",
      "INFO:tensorflow:loss = 12.278628, step = 1001 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 731.78\n",
      "INFO:tensorflow:loss = 16.401402, step = 1101 (0.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 589.552\n",
      "INFO:tensorflow:loss = 12.128653, step = 1201 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 753.775\n",
      "INFO:tensorflow:loss = 14.493934, step = 1301 (0.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 768.607\n",
      "INFO:tensorflow:loss = 14.86763, step = 1401 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 789.559\n",
      "INFO:tensorflow:loss = 12.53737, step = 1501 (0.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 725.155\n",
      "INFO:tensorflow:loss = 10.348017, step = 1601 (0.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 763.852\n",
      "INFO:tensorflow:loss = 15.309954, step = 1701 (0.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 783.227\n",
      "INFO:tensorflow:loss = 11.788655, step = 1801 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 668.834\n",
      "INFO:tensorflow:loss = 14.213116, step = 1901 (0.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 786.086\n",
      "INFO:tensorflow:loss = 12.597731, step = 2001 (0.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 696.89\n",
      "INFO:tensorflow:loss = 10.500768, step = 2101 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 729.523\n",
      "INFO:tensorflow:loss = 11.156, step = 2201 (0.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 787.916\n",
      "INFO:tensorflow:loss = 10.620663, step = 2301 (0.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 660.187\n",
      "INFO:tensorflow:loss = 11.264418, step = 2401 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 723.579\n",
      "INFO:tensorflow:loss = 10.298128, step = 2501 (0.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 755.427\n",
      "INFO:tensorflow:loss = 8.634987, step = 2601 (0.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 747.288\n",
      "INFO:tensorflow:loss = 10.298065, step = 2701 (0.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 726.031\n",
      "INFO:tensorflow:loss = 9.040804, step = 2801 (0.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 674.028\n",
      "INFO:tensorflow:loss = 13.123592, step = 2901 (0.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 748.685\n",
      "INFO:tensorflow:loss = 12.207674, step = 3001 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 772.648\n",
      "INFO:tensorflow:loss = 9.990184, step = 3101 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 709.174\n",
      "INFO:tensorflow:loss = 9.47152, step = 3201 (0.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 770.239\n",
      "INFO:tensorflow:loss = 11.595061, step = 3301 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 681.489\n",
      "INFO:tensorflow:loss = 9.276533, step = 3401 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 680.986\n",
      "INFO:tensorflow:loss = 9.116513, step = 3501 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 757.683\n",
      "INFO:tensorflow:loss = 9.563774, step = 3601 (0.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 797.8\n",
      "INFO:tensorflow:loss = 11.892941, step = 3701 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 623.792\n",
      "INFO:tensorflow:loss = 11.6252575, step = 3801 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 659.955\n",
      "INFO:tensorflow:loss = 9.292521, step = 3901 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 772.133\n",
      "INFO:tensorflow:loss = 9.3047905, step = 4001 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 809.843\n",
      "INFO:tensorflow:loss = 13.158644, step = 4101 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 756.233\n",
      "INFO:tensorflow:loss = 9.00252, step = 4201 (0.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 778.538\n",
      "INFO:tensorflow:loss = 9.615002, step = 4301 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 732.747\n",
      "INFO:tensorflow:loss = 8.524413, step = 4401 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 794.085\n",
      "INFO:tensorflow:loss = 9.121013, step = 4501 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 769.315\n",
      "INFO:tensorflow:loss = 7.4270725, step = 4601 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 782.239\n",
      "INFO:tensorflow:loss = 9.748638, step = 4701 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 571.649\n",
      "INFO:tensorflow:loss = 10.184966, step = 4801 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 731.93\n",
      "INFO:tensorflow:loss = 10.372631, step = 4901 (0.137 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/tmp9cv7ngir/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.830989.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifier at 0x7f99d7890160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
    "    steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-31T18:30:54Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/jugs/PycharmProjects/myvenv/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp9cv7ngir/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-31-18:30:54\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.96666664, average_loss = 0.081359774, global_step = 5000, loss = 2.4407933\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /tmp/tmp9cv7ngir/model.ckpt-5000\n",
      "\n",
      "Test set accuracy: 0.967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(test, test_y, training=False))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the model\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "def input_fn(features, batch_size=256):\n",
    "    \"\"\"An input function for prediction.\"\"\"\n",
    "    # Convert the inputs to a Dataset without labels.\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda: input_fn(predict_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp9cv7ngir/model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Prediction is \"Setosa\" (99.8%), expected \"Setosa\"\n",
      "Prediction is \"Versicolor\" (100.0%), expected \"Versicolor\"\n",
      "Prediction is \"Virginica\" (100.0%), expected \"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print('Prediction is \"{}\" ({:.1f}%), expected \"{}\"'.format(\n",
    "        SPECIES[class_id], 100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'sepal_length': tf.placeholder(tf.float32, [None, 1]),\n",
    "        'sepal_width': tf.placeholder(tf.float32, [None, 1]),\n",
    "        'petal_length': tf.placeholder(tf.float32, [None, 1]),\n",
    "        'petal_width': tf.placeholder(tf.float32, [None, 1]),\n",
    "    }\n",
    "\n",
    "    # Convert give inputs to adjust to the model.\n",
    "    features = {\n",
    "        INPUT_FEATURE: tf.concat([\n",
    "            receiver_tensors['sepal_length'],\n",
    "            receiver_tensors['sepal_width'],\n",
    "            receiver_tensors['petal_length'],\n",
    "            receiver_tensors['petal_width']\n",
    "        ], axis=1)\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(receiver_tensors=receiver_tensors,\n",
    "                                                    features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'serving_input_receiver_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-356d7156a5cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/iris\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserving_input_receiver_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserving_input_receiver_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'serving_input_receiver_fn' is not defined"
     ]
    }
   ],
   "source": [
    "classifier.export_saved_model(\"/tmp/iris\", serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
