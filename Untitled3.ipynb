{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy\n",
    "\n",
    "from deepspeaker.data_generators.sample_data_generator import SampleDataGenerator\n",
    "from deepspeaker.models.speaker_recognition.baseline_cnn import baseline_cnn\n",
    "from deepspeaker.models.speaker_recognition.baseline_dnn import baseline_dnn\n",
    "from deepspeaker.models.speaker_recognition.baseline_rnn import baseline_rnn\n",
    "from deepspeaker.utils.generic_utils import LoggingCallback, initialize_logger\n",
    "from deepspeaker.utils.generic_utils import list_files\n",
    "from deepspeaker.utils.generic_utils import mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_trainer(model, data_generator, epochs=100, workers=2, runs_path=None):\n",
    "    model.compile(optimizer='adam', loss=categorical_crossentropy, metrics=[categorical_accuracy])\n",
    "    train_gen = data_generator.generator(\"train\")\n",
    "    validation_gen = data_generator.generator(\"validation\")\n",
    "    test_gen = data_generator.generator(\"test\")\n",
    "    \n",
    "    if runs_path is None:\n",
    "       runs_path = os.path.join('.', 'runs_dir', str(int(time.time())))\n",
    "       logging.info(\"Runs path: {}\".format(runs_path))\n",
    "\n",
    "    mkdir(runs_path, dirname=True)\n",
    "    mkdir(os.path.join(runs_path, 'models'), dirname=True)\n",
    "    mkdir(os.path.join(runs_path, 'logs'), dirname=True)\n",
    "    \n",
    "    # Callbacks\n",
    "    weights_path = os.path.join(runs_path, 'models', 'weights.{epoch:03d}-{val_loss:.3f}.hdf5')\n",
    "    checkpointer = ModelCheckpoint(filepath=weights_path, verbose=0, save_best_only=False)\n",
    "    tensorboard = TensorBoard(log_dir=os.path.join(runs_path, 'logs'), write_images=True)\n",
    "    # lc = LoggingCallback()\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, filename=os.path.join(runs_path, 'log.txt'))\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        train_gen, steps_per_epoch=data_generator.train_spe, epochs=epochs,\n",
    "        validation_data=validation_gen, workers=workers, validation_steps=data_generator.validation_spe,\n",
    "        callbacks=[checkpointer, tensorboard]\n",
    "    )\n",
    "    pkl.dump(history.history, open(os.path.join(runs_path, 'training_history.pkl'), 'w'))\n",
    "\n",
    "    # select best model and evaluate on test data\n",
    "    weights = list_files(os.path.join(runs_path, 'models'), lambda x: x.endswith('hdf5'))\n",
    "    best_model_weight = sorted(weights, key=lambda x: float(x.split('-')[1][:-5]))[0]\n",
    "    model.load_weights(filepath=best_model_weight)\n",
    "\n",
    "    evaluation = model.evaluate_generator(test_gen, steps=data_generator.test_spe)\n",
    "    # make sure to stop all threads\n",
    "    data_generator.clean_up()\n",
    "\n",
    "    logging.info(\"Evaluation on test data: Loss: {}, Accuracy: {}\".format(evaluation[0], evaluation[1]))\n",
    "    logging.info(\"Model Training Completed\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]\n",
      "                             [--model_type {dnn,rnn,cnn}] [--epochs EPOCHS]\n",
      "                             [--num_threads NUM_THREADS]\n",
      "                             train_tfrecord_file validation_tfrecord_file\n",
      "                             test_tfrecord_file\n",
      "ipykernel_launcher.py: error: the following arguments are required: validation_tfrecord_file, test_tfrecord_file\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jugs/anaconda3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "    np.random.rand(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    session = K.get_session()\n",
    "\n",
    "    now = int(time.time())\n",
    "    runs_dir = os.path.join('.', 'runs_dir', str(now))\n",
    "    mkdir(runs_dir, dirname=True)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Run sample models on TIMIT/VCTK data')\n",
    "    parser.add_argument('train_tfrecord_file', type=str, help=\"Train TfRecord file path\")\n",
    "    parser.add_argument('validation_tfrecord_file', type=str, help=\"Validation TfRecord file path\")\n",
    "    parser.add_argument('test_tfrecord_file', type=str, help=\"Test TfRecord file path\")\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help=\"Batch size\")\n",
    "    parser.add_argument('--model_type', type=str, default='dnn', help=\"Type of model to run\",\n",
    "                        choices=['dnn', 'rnn', 'cnn'])\n",
    "    parser.add_argument('--epochs', type=int, default=100, help=\"Number of epochs to train\")\n",
    "    parser.add_argument('--num_threads', type=int, default=4, help=\"Number of threads for enqueue operation\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_generator = SampleDataGenerator(\n",
    "        train_filenames=args.train_tfrecord_file,\n",
    "        validation_filenames=args.validation_tfrecord_file,\n",
    "        test_filenames=args.test_tfrecord_file,\n",
    "        batch_size=args.batch_size,\n",
    "        model_type=args.model_type,\n",
    "        num_threads=args.num_threads\n",
    "    )\n",
    "\n",
    "    if args.model_type == 'dnn':\n",
    "        model = baseline_dnn(data_generator.shape, data_generator.num_labels)\n",
    "    elif args.model_type == 'cnn':\n",
    "        model = baseline_cnn(data_generator.shape, data_generator.num_labels)\n",
    "    else:\n",
    "        model = baseline_rnn(data_generator.shape, data_generator.num_labels)\n",
    "\n",
    "    initialize_logger(runs_dir)\n",
    "\n",
    "    sample_trainer(\n",
    "        model=model,\n",
    "        data_generator=data_generator,\n",
    "        epochs=args.epochs,\n",
    "        workers=2,\n",
    "        runs_path=runs_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
